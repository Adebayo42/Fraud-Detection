{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fraud Detection Model Training Script\n",
    "\n",
    "This script contains a structured pipeline for fraud detection using multiple machine learning classifiers.\n",
    "\"\"\"\n",
    "\n",
    "# Install necessary libraries (uncomment if needed)\n",
    "# %pip install imbalanced-learn category_encoders scikit-learn xgboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import necessary libraries\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import StackingClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read files\n",
    "train_transaction_v0 = pd.read_csv(\"./train_transaction.csv\")\n",
    "train_identity = pd.read_csv(\"./train_identity.csv\")\n",
    "test_transaction_v0 = pd.read_csv(\"./test_transaction.csv\")\n",
    "test_identity = pd.read_csv(\"./test_identity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merge the transaction and the identify dataset\n",
    "train_transaction = pd.merge(train_transaction_v0, train_identity, on=\"TransactionID\", how=\"left\")\n",
    "\n",
    "test_transaction = pd.merge(test_transaction_v0, test_identity, on=\"TransactionID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of duplicated records is: 0\n",
      "Count of duplicated records is: 0\n",
      "Shape of dataset is: 590,540 rows and 434 columns\n",
      "Shape of dataset is: 144,233 rows and 41 columns\n"
     ]
    }
   ],
   "source": [
    "## Check for duplicated records\n",
    "for df in [train_transaction,train_identity]:\n",
    "    duplicate = df.duplicated().sum()\n",
    "    print(f\"Count of duplicated records is: {duplicate}\")\n",
    "    \n",
    "## Check for dataset shape\n",
    "for df in [train_transaction,train_identity]:\n",
    "    shape = df.shape\n",
    "    print(f\"Shape of dataset is: {shape[0]:,} rows and {shape[1]:,} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Separate the input and target features\n",
    "features = train_transaction.drop(columns=\"isFraud\")\n",
    "target = train_transaction[[\"isFraud\"]]\n",
    "\n",
    "### Split the transaction data using 80% trainset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, target, train_size=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Class\n",
    "\n",
    "## create custom class to transform the transaction date column to Timedelta\n",
    "class tran_dt(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, col = \"TransactionDT\"):\n",
    "        self.col=col\n",
    "    \n",
    "    def fit(self, X, Y=None):\n",
    "        \"\"\"Fit does nothing as no learning is required\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X_new = X.copy()\n",
    "        X_new[self.col] = (X_new[self.col]/(24*60*60)).astype(\"float\")\n",
    "        return X_new\n",
    "    \n",
    "    \n",
    "## create custom class to drop High NAs columns in train_transaction\n",
    "class drop_na(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold = 0.2, ID_col = \"TransactionID\"):\n",
    "        self.threshold = threshold\n",
    "        self.ID_col = ID_col\n",
    "        self.column_to_drop = []\n",
    "        \n",
    "    def fit(self, X, Y=None):\n",
    "        self.column_to_drop = [self.ID_col] + [\n",
    "            col for col in X.columns if X[col].isna().sum()>= self.threshold * len(X)\n",
    "            ]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        X_new = X_new.drop(columns = self.column_to_drop, errors =\"ignore\")\n",
    "        return X_new\n",
    "    \n",
    "## create custom class to treat the missing values in the P_emaildomain using \"nomail.com\"\n",
    "class email_na(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, col = \"P_emaildomain\"):\n",
    "        self.col=col\n",
    "    \n",
    "    def fit(self, X, Y=None):\n",
    "        \"\"\"Fit does nothing as no learning is required\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X):\n",
    "        X_new = X.copy()\n",
    "        X_new.loc[X_new[self.col].isna(), self.col] = \"nomail.com\"\n",
    "        return X_new\n",
    "        \n",
    "\n",
    "## create custom class to treat the missing values in the add1|2 using 0\n",
    "class addr_na_handler(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, cols=[\"addr1\", \"addr2\"]):\n",
    "        self.cols = cols\n",
    "        \n",
    "    def fit(self, X, Y=None):\n",
    "        \"\"\"Fit does nothing as no learning is required\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        for col in self.cols:\n",
    "            X_new.loc[X_new[col].isna(), col] = 0.0\n",
    "        return X_new\n",
    "\n",
    " \n",
    "\n",
    "## Create a custum class to treat missing values\n",
    "class imputer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.non_numeric_col = None\n",
    "        self.numeric_col = None\n",
    "        self.mode_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "        self.median_imputer = SimpleImputer(strategy=\"median\")\n",
    "        \n",
    "    def fit(self, X, Y=None):\n",
    "        self.non_numeric_col = X.select_dtypes(exclude=[np.number]).columns\n",
    "        self.numeric_col = X.select_dtypes(include=[np.number]).columns\n",
    "        self.mode_imputer.fit(X[self.non_numeric_col])\n",
    "        self.median_imputer.fit(X[self.numeric_col])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        X_new[self.numeric_col] = self.median_imputer.transform(X[self.numeric_col])\n",
    "        X_new[self.non_numeric_col] = self.mode_imputer.transform(X[self.non_numeric_col])\n",
    "        return X_new\n",
    "    \n",
    "## Create a custom class to encode categorical variables\n",
    "class encoder(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.non_numeric_cols = None\n",
    "        self.encoder = None\n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        self.non_numeric_cols = X.select_dtypes(exclude=[np.number]).columns\n",
    "        self.encoder = ce.TargetEncoder(cols=self.non_numeric_cols)\n",
    "        self.encoder.fit(X[self.non_numeric_cols], Y)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        X_new = X.copy()\n",
    "        X_new[self.non_numeric_cols] = self.encoder.transform(X[self.non_numeric_cols])\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Development\n",
    "\n",
    "# Create pipeline for the randomforest classifier\n",
    "pipeline_rfc = make_pipeline(tran_dt(), drop_na(),email_na(),\n",
    "                         addr_na_handler(),imputer(),encoder(),\n",
    "                         SMOTE(random_state=42), StandardScaler(),\n",
    "                         RandomForestClassifier(n_estimators=100,random_state=42, n_jobs=-1))\n",
    "\n",
    "# Create pipeline for the logistic classifier\n",
    "pipeline_lgr = make_pipeline(tran_dt(), drop_na(),email_na(),\n",
    "                         addr_na_handler(),imputer(),encoder(),\n",
    "                         SMOTE(random_state=42), StandardScaler(),\n",
    "                         LogisticRegression( solver=\"saga\"))\n",
    "\n",
    "# Create pipeline for the GradientBoost classifier\n",
    "pipeline_gb = make_pipeline(tran_dt(), drop_na(),email_na(),\n",
    "                         addr_na_handler(),imputer(),encoder(),\n",
    "                         SMOTE(random_state=42), StandardScaler(),\n",
    "                         GradientBoostingClassifier(random_state=42))\n",
    "\n",
    "# Create pipeline for the HistGradientBoost classifier\n",
    "pipeline_hgb = make_pipeline(tran_dt(), drop_na(),email_na(),\n",
    "                         addr_na_handler(),imputer(),encoder(),\n",
    "                         SMOTE(random_state=42), StandardScaler(),\n",
    "                         HistGradientBoostingClassifier(random_state=42))\n",
    "\n",
    "# Create pipeline for the xgboost classifier\n",
    "pipeline_xgb = make_pipeline(tran_dt(), drop_na(threshold=1.1),email_na(),\n",
    "                         addr_na_handler(),imputer(),encoder(),\n",
    "                         SMOTE(random_state=42), StandardScaler(),\n",
    "                         xgb.XGBClassifier(learning_rate=0.01,\n",
    "                                           eval_metric =\"auc\",random_state=42,subsample=1,\n",
    "                                           use_label_encoder=False, n_jobs=-1))\n",
    "\n",
    "## Stacking Ensemble\n",
    "st_clf = StackingClassifier(\n",
    "    estimators=[(\"rf_clf\",pipeline_rfc), (\"logistic\",pipeline_lgr),\n",
    "                (\"GradientBoost\",pipeline_gb),(\"HistGB\",pipeline_hgb),\n",
    "                (\"xgb_cfl\", pipeline_xgb)],\n",
    "    final_estimator= lgb.LGBMClassifier()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:17:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:93: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:129: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [11:58:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:29:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:31:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:32:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:33:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [14:35:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 16567, number of negative: 455865\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003125 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1121\n",
      "[LightGBM] [Info] Number of data points in the train set: 472432, number of used features: 5\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.035067 -> initscore=-3.314784\n",
      "[LightGBM] [Info] Start training from score -3.314784\n"
     ]
    }
   ],
   "source": [
    "## Fit the models\n",
    "\n",
    "for model in [pipeline_rfc,pipeline_lgr,pipeline_gb, pipeline_hgb,pipeline_xgb, st_clf]:\n",
    "    model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Randomforest model is 0.980171\n",
      "ROC of Randomforest model is 0.919911\n",
      "Accuracy of Logistic model is 0.754597\n",
      "ROC of Logistic model is 0.822420\n",
      "Accuracy of GradientBoost model is 0.943907\n",
      "ROC of GradientBoost model is 0.875694\n",
      "Accuracy of HistGB model is 0.973558\n",
      "ROC of HistGB model is 0.902777\n",
      "Accuracy of XGBoost model is 0.938929\n",
      "ROC of XGBoost model is 0.861592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stacked model is 0.981043\n",
      "ROC of Stacked model is 0.935505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adeba\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "models_ = {\"Randomforest\":pipeline_rfc, \"Logistic\":pipeline_lgr,\n",
    "           \"GradientBoost\":pipeline_gb, \"HistGB\":pipeline_hgb,\n",
    "           \"XGBoost\":pipeline_xgb, \"Stacked\":st_clf}\n",
    "\n",
    "for key, value in models_.items():\n",
    "    pred = value.predict(X_test)\n",
    "    pred_prob = value.predict_proba(X_test)\n",
    "    acc = accuracy_score(Y_test, pred)\n",
    "    auc = roc_auc_score(Y_test,pred_prob[:,1])\n",
    "    print(f\"Accuracy of {key} model is {acc:.6f}\")\n",
    "    print(f\"ROC of {key} model is {auc:.6f}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename test columns to fix the name issue\n",
    "\n",
    "## create a name dictionary\n",
    "col_names_dict = dict(zip(test_transaction.columns, train_transaction.drop(columns=[\"isFraud\"]).columns))\n",
    "\n",
    "## Rename the test data columns\n",
    "test_data = test_transaction.rename(columns=col_names_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Make prediction with the test data\n",
    "\n",
    "prediction = pipeline_hgb.predict_proba(test_data)\n",
    "\n",
    "## create a submission dataframe\n",
    "submission_file = test_data[[\"TransactionID\"]]\n",
    "\n",
    "## Add the prediction to the submission dataframe\n",
    "submission_file[\"isFraud\"] = pd.DataFrame(prediction[:,1].round(1))\n",
    "\n",
    "## Save submission file to computer\n",
    "submission_file.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
